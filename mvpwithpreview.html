<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Transcription Companion</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            max-width: 800px;
            margin: 0 auto;
            background-color: #f0f4f8;
            display: flex;
            flex-direction: column;
            align-items: center;
            padding: 20px;
        }
        .container {
            display: flex;
            flex-direction: column;
            align-items: center;
            background-color: white;
            border-radius: 15px;
            box-shadow: 0 10px 25px rgba(0,0,0,0.1);
            padding: 20px;
            width: 100%;
        }
        .media-container {
            display: flex;
            justify-content: space-between;
            width: 100%;
            gap: 20px;
        }
        #videoPreview, #capturedImage {
            width: 48%;
            max-width: 640px;
            height: 480px;
            background-color: #000;
            border-radius: 10px;
            object-fit: cover;
        }
        #recordButton, #askButton {
            background-color: #4CAF50;
            border: none;
            color: white;
            padding: 15px 32px;
            text-align: center;
            font-size: 16px;
            margin: 20px 0;
            cursor: pointer;
            border-radius: 8px;
            transition: all 0.3s ease;
        }
        #recordButton.recording {
            background-color: #f44336;
            animation: pulse 1s infinite;
        }
        #transcription {
            width: 100%;
            max-width: 640px;
            background-color: #e9ecef;
            border: 2px dashed #3498db;
            padding: 20px;
            margin-top: 20px;
            border-radius: 8px;
            min-height: 100px;
            text-align: center;
        }
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.05); }
            100% { transform: scale(1); }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="media-container">
            <video id="videoPreview" autoplay playsinline></video>
            <canvas id="capturedImage" style="display:none;"></canvas>
        </div>
        <button id="recordButton">Start Recording</button>
        <button id="askButton">Ask</button>
        <div id="transcription">
            Transcription will appear here...
        </div>
    </div>

    <script>
        class ContextualTranscription {
            constructor() {
                this.videoPreview = document.getElementById('videoPreview');
                this.capturedImage = document.getElementById('capturedImage');
                this.recordButton = document.getElementById('recordButton');
                this.askButton = document.getElementById('askButton');
                this.transcriptionDiv = document.getElementById('transcription');
                
                // Speech Recognition Setup
                this.recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
                
                // Stream and capture variables
                this.mediaStream = null;
                
                // Initialize components
                this.setupCameraPreview();
                this.setupSpeechRecognition();
                this.setupAskButton();
            }

            async setupCameraPreview() {
                try {
                    this.mediaStream = await navigator.mediaDevices.getUserMedia({
                        video: { 
                            width: { ideal: 640 },
                            height: { ideal: 480 }
                        },
                        audio: false
                    });
                    this.videoPreview.srcObject = this.mediaStream;
                } catch (error) {
                    this.handleError(`Camera Error: ${error.message}`);
                }
            }

            setupSpeechRecognition() {
                this.recognition.continuous = false;
                this.recognition.interimResults = false;
                this.recognition.lang = 'en-US';

                this.recognition.onstart = () => {
                    this.recordButton.textContent = 'Recording...';
                    this.recordButton.classList.add('recording');
                };

                this.recognition.onresult = (event) => {
                    const transcript = event.results[0][0].transcript;
                    this.transcriptionDiv.textContent = transcript;
                    this.transcriptionDiv.style.color = '#2c3e50';
                    
                    // Capture image after successful transcription
                    this.captureImage();
                };

                this.recognition.onerror = (event) => {
                    this.handleError(`Error: ${event.error}`);
                };

                this.recognition.onend = () => {
                    this.resetButton();
                };

                this.recordButton.onclick = () => this.toggleRecording();
            }

            setupAskButton() {
                this.askButton.onclick = () => this.askLlama();
            }

            captureImage() {
                try {
                    const ctx = this.capturedImage.getContext('2d');
                    this.capturedImage.width = this.videoPreview.videoWidth;
                    this.capturedImage.height = this.videoPreview.videoHeight;
                    ctx.drawImage(this.videoPreview, 0, 0, 
                        this.capturedImage.width, 
                        this.capturedImage.height
                    );
                    this.capturedImage.style.display = 'block';
                    
                    // Resize the image
                    this.resizeImage(this.capturedImage, 320, 240).then(resizedImageData => {
                        this.capturedImageData = resizedImageData;
                    });
                } catch (error) {
                    this.handleError(`Image Capture Error: ${error.message}`);
                }
            }

            resizeImage(image, width, height) {
                return new Promise((resolve) => {
                    const canvas = document.createElement('canvas');
                    canvas.width = width;
                    canvas.height = height;
                    const ctx = canvas.getContext('2d');
                    ctx.drawImage(image, 0, 0, width, height);
                    resolve(canvas.toDataURL('image/jpeg'));
                });
            }

            async askLlama() {
                const imageData = this.capturedImageData || this.capturedImage.toDataURL('image/jpeg');
                const transcription = this.transcriptionDiv.textContent || "Temporary transcription data";
                
                // Send to Llama
                this.sendToLlama(imageData, transcription)
                    .then(response => {
                        // Handle the Llama model response
                        this.transcriptionDiv.textContent += '\n\nLlama Response: ' + JSON.stringify(response, null, 2);
                    });
            }

            async sendToLlama(imageData, transcription) {
                const HF_TOKEN = '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'; // Replace with your actual token
                
                // Reduce the size of the image data
                const compressedImageData = imageData.split(',')[1].substring(0, 1000); // Example: truncate the base64 string

                // Truncate the transcription text
                const truncatedTranscription = transcription.substring(0, 1000); // Example: limit to 1000 characters

                try {
                    const response = await fetch(
                        "https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-11B-Vision-Instruct",
                        {
                            headers: {
                                Authorization: `Bearer ${HF_TOKEN}`,
                                'Content-Type': 'application/json'
                            },
                            method: "POST",
                            body: JSON.stringify({
                                inputs: `${truncatedTranscription} [IMAGE] ${compressedImageData}` // Combine text and image data into a single string
                            }),
                        }
                    );

                    if (!response.ok) {
                        const errorText = await response.text();
                        throw new Error(`HTTP error! status: ${response.status}, message: ${errorText}`);
                    }

                    const contentType = response.headers.get("content-type");
                    if (contentType && contentType.indexOf("application/json") !== -1) {
                        const result = await response.json();
                        return result;
                    } else {
                        const text = await response.text();
                        throw new Error(`Unexpected response format: ${text}`);
                    }
                } catch (error) {
                    this.handleError(`Llama API Error: ${error.message}`);
                }
            }

            toggleRecording() {
                try {
                    if (this.recognition.listening) {
                        this.recognition.stop();
                    } else {
                        this.recognition.start();
                    }
                } catch (error) {
                    this.handleError('Speech recognition not supported');
                }
            }

            handleError(message) {
                this.transcriptionDiv.textContent = message;
                this.transcriptionDiv.style.color = '#e74c3c';
                this.resetButton();
            }

            resetButton() {
                this.recordButton.textContent = 'Start Recording';
                this.recordButton.classList.remove('recording');
                this.recognition.listening = false;
            }
        }

        // Initialize on page load
        document.addEventListener('DOMContentLoaded', () => {
            if (!('SpeechRecognition' in window || 'webkitSpeechRecognition' in window)) {
                alert('Your browser does not support speech recognition. Try Chrome or Edge.');
                return;
            }
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                alert('Your browser does not support camera access.');
                return;
            }
            new ContextualTranscription();
        });
    </script>
</body>
</html>